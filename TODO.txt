!run neuralpde with the old non experiment manager interface on the spm model generated 
    (programmatically fix the model possibly? I think I need to expand the derivatives so I need to figure out how to do that cleanly again.)
    ^ wasn't necessary

!Finish hooking up the experiment manager to the SPM example to finish the entire loop from 
    pybamm model spec -> (pybamm model gen + pybamm sim) -> NeuralPDE nn train -> compare pybamm sim to nn eval
    for the spm

!finish homogeneous mdf applied to diffeqflux fastchains

!make mdf heterogeneous

!from there, replace the neuralpde nn stuff with the multidimensional function interface I've been designing (put it in this repo for now for simplicity)

!start on next model

spme:
!fixed a lot of mtk generation stuff
!added correct tracking variable
!add dependent variable dependencies to a dict for easier parsing
!make the sim parsing go in reverse tensor order in general (since they use numpy's ordering I think)
!generalize parsing for any order of array

!get IfElse.ifelse to broadcast correctly
!add x to the concentration function generation in pybamm
!redo plotting and logging to loop over the given data

!figure out how to get pybamm to also output the variable values on the edge of the domains (needed for model eval/debugging)
  !add a ghost node to it from my side? I don't know that pybamm will allow me to add ghost nodes to neumann even though I want it 
    ! no, I figured out how to call the solution variable directly, and it does a very small amount of extrapolation

!add a num_pts input to the simulation to control fidelity

!evaluate the generated losses on the sim data (using the call functions) to validate that the equations are correct (IMPORTANT!)
    !need to make the interpolation higher order because the linear interpolation is failing on the 2nd order stuff
    !finished for spm
    !need to generalize this process and add the scale dependence
    !on a progression of num_pts resolutions to show that the loss is decreasing as resolution increases (up to a point)
        !(not doing, just testing that we get very low error on high res)
    still need to do for 3d dvs (need to figure out what the fourth scipy.interpolate variable is named after :z)


!add a simpler ifelse test case 
    !this works, so why doesn't the larger one?
    is it the derivatives through the Dx(concentration)??
        try this in the easy test case

!spme and beyond: isn't generating the boundary conditions for eps_c_e
    !this is because the boundary conditions are specified for c_e and not eps_c_e so the transpiler isn't noting them.
        !can I have it convert at the end? how does it know that it needs to convert that equation?
        !c_e is a Concentration and eps_c_e is a ConcentrationVariable, that's the main way it was distinguishing to write or not
        !but there are other Concentrations that I don't want to save. how do I know to save the c_e one?
    !I ended up making c_e be the primary variable and then everything works since it reads from c_e instead now. hooray!

!spme: why does the generated loss do so poorly?
    !try diving into each concatenation and doing the variables within each domain separately and then only doing one final concatenation at the end.
        !this 
    !strangely, the loss does fine in the separator region (or, most of it)????
    !the loss does really bad after the separator up to like 0.66
    !oops, the interpolator is incorrectly scaled in x direction because of the different lengths of the regions. gonna do a really high res interpolation and then resample it w/ lower freq
    !yay after rescaling it performs much better (aside from the transients at the sep boundary which the interpolation doesn't track well)
    !can we generate the pde eqs in terms of just concentration? the porosity makes it super high freq at the boundaries but we don't want that.
        !yes, we should do this. this might fix the issue for the neumann condition too
    yes, that works!  the loss is still pretty bad on the interpolation though.  can I have it use the sub-interpolations for each of the regions??
        this might be a micro optimization, should get it running on supercloud first.

!things to do to get it to run on supercloud:
    !make it run locally:
        !keep the same data format for now but make it use interpolated high resolution values instead of the entire high resolution blob
        !make sure it works with the c_e versus eps_c_e change
        !add abs(error) to the error plot, it's more useful (can still plot regular error too)
        !remove the log printouts
        !run it for a lil bit to make sure it can do a cycle or two
        !added endat semantics to run a slice of the hyperparameter experiments
        !added a serial queue mode for experiment manager
        !make a slurm-style script !(made it able to run serial)
    !make it run on cloud:
        !update julia packages
        !update the NeuralPDE git repo
        !download DFNExperiments & the slurm folder
        !run hyperparameter experiment via slurm launcher

!make pybamm spit out the indvars in the same order each time

SPMnoR: why is it outputting an @ ??? this is a very simple model, maybe it's not reading the variable number correctly.
!ReducedC: it wants to get the 'Discharge capacity [A.h]' (Q_Ah in other models) but the sim doesn't want to include that. can I pass in only the sim 
    !variables to the sim object? currently the variables I pass in is a superset (just whatever variables the pycall generation script passes back to me)
    !added a variable about whether to include Q or not

DFN blocks:
    figure out integration domain of each equation and then use the correct variable there. this might interfere with the interpolators? can I just set it to extrapolation is fine so I don't get all the domain errors?
        !extrapolation handled by wrapping in an extrapolation() interpolation struct
        handle the x / xn / xp discrepancy again. I think last time I just put the variable that corresponded to the integration domain of the equation in there? 
            so how do I figure that out programmatically. 
            so the correct way to do this is to define "subdomains" and integration domains for each equation.  
                we can default to having 
    figure out saving 3d blobs for reuse and plotting 3d blobs (basically just make sure both work still)



!ok just pick the subdomain one. it's "doing it right" and not a lot more work so let's go!
how to get the info to the neuralpde parser:
    the correct way is to include it in the PDESystem. that takes a more work so I'm just going to include it as kwargs in the discretization.
        !when this gets merged I will do it correctly but for the demo I will not
    the way I'm doing it is by passing in subdomain relations as a Dict{Symbol, Symbol} of subdomain -> domain for those symbols
    and the equation integration domains as a vector of tuples of the Symbols of integration
    if either is nothing then default to the old behavior within neuralpde

how to get the info out of PyBaMM:
    subdomain:
        if x_n, x_s, or x_p and x are in the variables we care about, then 
            add the subdomain relation to the end
            this won't work for larger stuff but we can hardcode it for now like the concatenation stuff is hardcoded

    integration vars:
        look at rhs's to figure out the integration domain
            does this interact poorly with the Dx_n stuff in the bc's, for instance???

within neuralpde parser/transformer:
    pass the subdomain & integration info to the build symbolic stuff
    dig in there to figure out exactly where things need to change. it's probably in the first sections where variables are assigned (in particular, cord 's)

get it to work within neuralpde first, then change the transpiler second 
    (less important, since I can manually add it for now. if I do it correctly, the loss_certificate should go down once I make the manual change and then I can automate it)

    !get it to run inside neuralpde, 
        then piece it apart
    !get the info inside w/ kwargs
        !symbolic_discretize first (make sure symbolic_loss_function accepts the right args and gets the integration domains correctly)
        !discretize after (make sure it aligns with the symbolic_discretize version)
    !surgical changes
        !make the let (t, x_p, x, x_n) bit use the correct input
        !within symbolic_loss:
            !if integration domain is x_n and x and x_n are both in the eq, then do let x = cord[[index(x_n)], :]
            !if integration domain is x and x and x_n are both in the eq, then do let x_n = cord[[index(x)], :]
            !counting the cords in symbolic loss: generate eq_integration_domain if it's nothing, from the pairs, and use that instead
        !outside of the symbolic loss:
            !make sure that the bounds are derived from integration_domains, not the way they were before
                !get_bounds for everything except for grid
                    !prioritize this
                    !lol it's dispatched on strategy, oops
                    !prioritize stochastictraining

                    !add a dispatch to get_argument that dispatches on eqs_integration_domains
                    !tested for stochastic with printouts, might break actual tests tho
                TODO: fill out get_bounds for quadraturetraining and generate_training_sets for gridstrategy
                    what is the difference between get_variables and get_arguments? critical to understanding the difference between quadraturetraining and others
                    generate_training_sets for grid
        
        !ok I think it's all done except BC handling is done incorrectly (gotta make sure the 0-dimensional BCs are fed in correctly, in the right spots)
            !in get_argument(bc_args...)
            !I handled this by putting the frozen dimensions into the integration domains for bcs too
        !haven't tested this with the loss for certification yet tho
            !cert is 10x lower than before at 0.5 ! I think that's low enough to qualify 

    !reduced_c_phi_j works fully now

    DFN_no_r
        manual annotation changes:
            !\kappa -> kappa
            !added the subdomain_relations
            !added eq_integration_domains: realized this could actually be calculated from the same place the dependent_variables_to_dependencies is, since it's the same data
            !added ics_bcs_integration_domains: this is harder to get out of pybamm
            I think it's missing the boundary conditions for c_s_n_rav and c_s_p_rav
                the sim.model doesn't have these, I guess it doesn't have them in general??? maybe an issue though. I don't know what they should be in general for x domain bcs for c_s_n and c_s_p

        things I noticed from examining the losses:
            equation 4 (c_e pde) has huge loss. this is consistent with the observations from spme (large loss spikes near the separator boundary)
            equation 12 (phi_s_p ic) has ~constant ~-3.13 loss.  this is strange and likely indicates a modelling issue
            equation 14 (phi_e ic) has reasonably large but nonconstant loss.  this is strange and likely indicates a modelling issue
            all other losses were small generally

    DFN
        !handle the data blobs being 3D for c_s_n and c_s_p
            !handled the RegularGridInterpolator
        !manual annotation changes:
            !\kappa -> kappa
            !c_s_n/p ic: 
                !didn't include the x_n and x_p variables
            !added the subdomain_relations
            !added eq_integration_domains: realized this could actually be calculated from the same place the dependent_variables_to_dependencies is, since it's the same data
            !added ics_bcs_integration_domains: this is harder to get out of pybamm
            !what to do about the c_s_n(t, 1.0, x_n) within pde issues????
                !the way I handled the integration domains for the previous two won't hold here. need to annotate which variable is getting the set value 
                !this will *not* work for all possible bcs because some pdes might have more than one fixed value for different parts of the equation 
                    !(c_s_n(t,0.0,x_n) + c_s_n(t,1.0,x_n) would fail, for instance)
    

    !changed all integration_domain manual annotations to include the variable that is being replaced
    !fixed the pde parser to recognize this format and fix it
    !dfn runs! the loss is very terrible initially but it at least runs through loss without error
    !sometimes the inits need tweaking because the sinh is out of domain
                


full DFN fix problems
    interpolation for better loss cert
finish implementing laplacian eigenfunction rMFN (with the sum of regular MLP for the nonzero boundary conditions)
exponential moving average of time domain weights
    implement as a sampling strategy? this could be easy and efficient
            
            
            
            






why is the boundary/initial condition for phi_s_p different than the other ones? ic of 2 and bc of (0, 1) neumann
    make sure the DFN one is similar

add kwargs handling to StructGenerator to set the initial relative weights for the losses
    work around for now is to just add a non-kwarg function to the MiniMaxAdaptiveLoss

then start integration with the other models and fix errors/ expand design as they come up
    ongoing! lots of progress tho

collapse the concatenation terms so that it's just one concatenation term and then all the logic is combined so we only have to do one ifelse per equation
    (not strictly necessary but it would be a great optimization and for ease of understanding)
    !probably unnecessary now that the loss functions have a good test


domain_decomposition stuff:

!get rid of \ in \kappa_e function name 
!make the caches in div/grad point to instantiated variables (did they used to? what did I mess up? they point to the old concat variables (c_e and phi_e), make them point to the right sub variable)
!add _n, _s, _p suffices to cache variable names that are in lines that contain a _n, _s, _p and make sure that all defined cache variable lhs' have the suffix if they need it
!does it work correctly for the previous models or just dfn?
!make FastChainInterpolator use a Lux object instead
investigate why the loss is high for SPM




