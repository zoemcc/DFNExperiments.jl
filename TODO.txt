!run neuralpde with the old non experiment manager interface on the spm model generated 
    (programmatically fix the model possibly? I think I need to expand the derivatives so I need to figure out how to do that cleanly again.)
    ^ wasn't necessary

!Finish hooking up the experiment manager to the SPM example to finish the entire loop from 
    pybamm model spec -> (pybamm model gen + pybamm sim) -> NeuralPDE nn train -> compare pybamm sim to nn eval
    for the spm

!finish homogeneous mdf applied to diffeqflux fastchains

!make mdf heterogeneous

!from there, replace the neuralpde nn stuff with the multidimensional function interface I've been designing (put it in this repo for now for simplicity)

!start on next model

spme:
!fixed a lot of mtk generation stuff
!added correct tracking variable
!add dependent variable dependencies to a dict for easier parsing
!make the sim parsing go in reverse tensor order in general (since they use numpy's ordering I think)
!generalize parsing for any order of array

!get IfElse.ifelse to broadcast correctly
!add x to the concentration function generation in pybamm
!redo plotting and logging to loop over the given data

!figure out how to get pybamm to also output the variable values on the edge of the domains (needed for model eval/debugging)
  !add a ghost node to it from my side? I don't know that pybamm will allow me to add ghost nodes to neumann even though I want it 
    ! no, I figured out how to call the solution variable directly, and it does a very small amount of extrapolation

!add a num_pts input to the simulation to control fidelity

!evaluate the generated losses on the sim data (using the call functions) to validate that the equations are correct (IMPORTANT!)
    !need to make the interpolation higher order because the linear interpolation is failing on the 2nd order stuff
    !finished for spm
    !need to generalize this process and add the scale dependence
    !on a progression of num_pts resolutions to show that the loss is decreasing as resolution increases (up to a point)
        !(not doing, just testing that we get very low error on high res)
    still need to do for 3d dvs (need to figure out what the fourth scipy.interpolate variable is named after :z)

spme: why does the generated loss do so poorly?

!add a simpler ifelse test case 
    !this works, so why doesn't the larger one?

!make pybamm spit out the indvars in the same order each time

SPMnoR: why is it outputting an @ ??? this is a very simple model, maybe it's not reading the variable number correctly.
ReducedC: it wants to get the 'Discharge capacity [A.h]' (Q_Ah in other models) but the sim doesn't want to include that. can I pass in only the sim 
    variables to the sim object? currently the variables I pass in is a superset (just whatever variables the pycall generation script passes back to me)


add kwargs handling to StructGenerator
then start integration with the other models and fix errors/ expand design as they come up

collapse the concatenation terms so that it's just one concatenation term and then all the logic is combined so we only have to do one ifelse per equation
    (not strictly necessary but it would be a great optimization and for ease of understanding)

handle the x / xn / xp discrepancy again. I think last time I just put the variable that corresponded to the integration domain of the equation in there? 
    so how do I figure that out programmatically. 


